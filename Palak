import time
import pandas as pd
import streamlit as st
import numpy as np
import plotly.express as px
import threading

# Set Streamlit page config
st.set_page_config(
    page_title="Log File Data",
    page_icon="âœ…",
    layout="wide",
)

# Streamlit styles
st.markdown(
    """
    <style>
        body { background-color: black; color: white; }
        .stApp { background-color: black; }
        .css-1d391kg p, .css-1offfwp, .css-1iukzno, .css-1kyxreq, .css-1v3fvcr, .css-1cpxqw2, .css-1aumxhk, .css-1inwz65, .css-1k0ckh2, .css-1v3fvcr, .css-1d2lz6h { color: white !important; }
        .css-10trblm { color: white !important; }
        .css-145kmo2, .css-8yuyfy, .css-2trqyj, .css-12ttj6m, .css-1rwmvkb { color: white !important; }
        h1, h2, h3, h4, h5, h6 { color: white !important; }
        .stTextInput label, .stDateInput label, .stTimeInput label, .stSelectbox label { color: white !important; }
        .kpi { display: flex; flex-direction: column; align-items: center; justify-content: center; color: white !important; }
        .kpi-value { font-size: 2.5em; font-weight: bold; }
        .kpi-delta { font-size: 1em; }
    </style>
    """,
    unsafe_allow_html=True
)

# Function to read and parse the log file
@st.cache_data
def load_data(log_file_path: str) -> pd.DataFrame:
    with open(log_file_path, 'r') as file:
        logs = file.readlines()
    
    # Extract fields from log entries
    data = []
    for log in logs:
        fields = {}
        parts = log.split()
        for part in parts:
            if "=" in part:
                key, value = part.split("=", 1)
                fields[key] = value.strip('"')
        data.append(fields)
    return pd.DataFrame(data)

log_file_path = '/mnt/data/disk-event-system-2024_05_30.txt'

# Load the data
df = load_data(log_file_path)

# Convert 'date' and 'time' to a datetime column for filtering
df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])

# Display the DataFrame
st.title("Live Log Analysis")
unique_levels = pd.unique(df["level"])
level_filter = st.selectbox("Select the level", unique_levels)

# Date and time inputs for filtering
col1, col2 = st.columns(2)
with col1:
    start_date = st.date_input("Start Date", value=pd.to_datetime(df['datetime']).min().date())
    start_time = st.time_input("Start Time", value=pd.to_datetime(df['datetime']).min().time())
with col2:
    end_date = st.date_input("End Date", value=pd.to_datetime(df['datetime']).max().date())
    end_time = st.time_input("End Time", value=pd.to_datetime(df['datetime']).max().time())

# Combine date and time inputs into datetime
start_datetime = pd.to_datetime(f"{start_date} {start_time}")
end_datetime = pd.to_datetime(f"{end_date} {end_time}")

# Function to update the log file with the current time every 5 seconds
def update_log_file(log_file_path: str):
    while True:
        current_time = time.strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f'date={current_time.split()[0]} time={current_time.split()[1]} level=info logdesc="Current Time" msg="Current time entry"\n'
        with open(log_file_path, 'a') as file:
            file.write(log_entry)
        time.sleep(5)

# Run the log file update in a separate thread
thread = threading.Thread(target=update_log_file, args=(log_file_path,))
thread.start()

# Placeholder for dynamic content
placeholder = st.empty()

for seconds in range(200):
    df = load_data(log_file_path)  # Reload data to include the latest log entries
    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])

    # Filter the DataFrame based on the selected level and date-time range
    df_filtered = df[(df["level"] == level_filter) & (df["datetime"] >= start_datetime) & (df["datetime"] <= end_datetime)]

    df_filtered["tz_new"] = df_filtered["tz"].astype(int)
    df_filtered["logid_new"] = df_filtered["logid"].astype(int)
    
    avg_tz = np.mean(df_filtered["tz_new"])
    count_DHCP_statistics = int(df_filtered[df_filtered["logdesc"] == "DHCP statistics"]["logdesc"].count() + np.random.choice(range(1, 30)))
    
    with placeholder.container():
        kpi1, kpi2 = st.columns(2)
        
        kpi1.markdown(
            f"""
            <div class="kpi">
                <div class="kpi-label">tz</div>
                <div class="kpi-value">{round(avg_tz)}</div>
                <div class="kpi-delta">{round(avg_tz) - 10}</div>
            </div>
            """,
            unsafe_allow_html=True
        )
        
        kpi2.markdown(
            f"""
            <div class="kpi">
                <div class="kpi-label">DHCP count</div>
                <div class="kpi-value">{int(count_DHCP_statistics)}</div>
                <div class="kpi-delta">{-10 + count_DHCP_statistics}</div>
            </div>
            """,
            unsafe_allow_html=True
        )
        
        fig_col1, fig_col2, fig_col3 = st.columns(3)
        
        with fig_col1:
            st.markdown("### First Chart")
            fig = px.density_heatmap(data_frame=df_filtered, y="tz_new", x="logdesc")
            fig.update_layout(plot_bgcolor="black", paper_bgcolor="black", font=dict(color="white"))
            st.write(fig)
        
        with fig_col3:
            st.markdown("### Third Chart")
            fig3 = px.histogram(data_frame=df_filtered, x="tz_new")
            fig3.update_layout(plot_bgcolor="black", paper_bgcolor="black", font=dict(color="white"))
            st.write(fig3)
        
        with fig_col2:
            st.markdown("### Second Chart")
            pie_chart_data = df_filtered['logdesc'].value_counts().reset_index()
            pie_chart_data.columns = ['logdesc', 'count']
            fig2 = px.pie(pie_chart_data, values='count', names='logdesc')
            fig2.update_layout(plot_bgcolor="black", paper_bgcolor="black", font=dict(color="white"))
            st.write(fig2)
        
        st.markdown("<h3 style='color: white;'>Detailed Data View</h3>", unsafe_allow_html=True)
        st.dataframe(df_filtered)
        
        time.sleep(1)
