import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
import os
import pytz
import psycopg2
from psycopg2 import sql

# API and Access Token URLs
API_URL = ""
ACCESS_TOKEN_URL = ""

def get_access_token():
    response = requests.post(ACCESS_TOKEN_URL)
    response_data = response.json()
    if "access_token" in response_data:
        print("Access Token generated successfully.")
        return response_data["access_token"]
    raise Exception("Failed to generate access token")

def fetch_data(start_date, end_date, auth_token):
    headers = {"Authorization": f"Bearer {auth_token}", "Content-Type": "application/json"}
    all_data = []
    top = 10000
    skip = 0
    while True:
        params = {"startdate": start_date, "enddate": end_date, "top": top, "skip": skip}
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()
        if "completedContacts" in data and data["completedContacts"]:
            all_data.extend(data["completedContacts"])
            skip += top
            if len(data["completedContacts"]) < top:
                break
        else:
            break
    print(f"Total records fetched: {len(all_data)}")
    return all_data

def save_raw_data_to_csv(df, dir_path):
    file_path = os.path.join(dir_path, "Raw_data.csv")
    df.to_csv(file_path, index=False)
    print(f"Raw data saved to '{file_path}' successfully!")
    return file_path

def clean_csv(csv_file_path):
    df = pd.read_csv(csv_file_path)
    
    # Drop the "tags" column if it exists.
    if "tags" in df.columns:
        df.drop(columns=["tags"], inplace=True)
        print("Dropped 'tags' column from CSV.")
    
    # List of datetime columns to clean and convert.
    datetime_columns = ["contactStartDate", "dateACWWarehoused", "dateContactWarehoused", "lastUpdateTime"]
    for col in datetime_columns:
        if col in df.columns:
            # Remove literal "T" and "Z" from the strings.
            df[col] = df[col].astype(str).str.replace("T", " ").str.replace("Z", "", regex=False)
            # Convert the cleaned string to a datetime object (assumes format: YYYY-MM-DD HH:MM:SS)
            df[col] = pd.to_datetime(df[col], format="%Y-%m-%d %H:%M:%S", errors="coerce")
            # Localize as UTC then convert to EST.
            df[col] = df[col].dt.tz_localize("UTC").dt.tz_convert("US/Eastern")
    df.to_csv(csv_file_path, index=False)
    print(f"CSV file cleaned and updated at '{csv_file_path}'")
    return df

def create_table_and_insert_data(db_conn, table_name, df):
    cursor = db_conn.cursor()
    columns = list(df.columns)
    # Define which columns are datetime so that they use TIMESTAMP; the rest will be TEXT.
    datetime_columns = ["contactStartDate", "dateACWWarehoused", "dateContactWarehoused", "lastUpdateTime"]
    column_defs = []
    for col in columns:
        if col in datetime_columns:
            col_def = sql.SQL("{} TIMESTAMP").format(sql.Identifier(col))
        else:
            col_def = sql.SQL("{} TEXT").format(sql.Identifier(col))
        column_defs.append(col_def)
    
    # Create table only if it does not exist.
    create_table_query = sql.SQL("CREATE TABLE IF NOT EXISTS {} (").format(sql.Identifier(table_name)) + \
                         sql.SQL(", ").join(column_defs) + sql.SQL(")")
    cursor.execute(create_table_query)
    print(f"Table '{table_name}' created (if it did not already exist).")
    
    # Insert data.
    for row in df.itertuples(index=False, name=None):
        insert_query = sql.SQL("INSERT INTO {} ({}) VALUES ({})").format(
            sql.Identifier(table_name),
            sql.SQL(", ").join(map(sql.Identifier, columns)),
            sql.SQL(", ").join(sql.Placeholder() * len(columns))
        )
        cursor.execute(insert_query, row)
    
    db_conn.commit()
    cursor.close()
    print(f"Data inserted into table '{table_name}' successfully!")

def delete_csv(file_path):
    os.remove(file_path)
    print(f"CSV file '{file_path}' deleted successfully!")

if __name__ == "__main__":
    # Create a Temp directory under the current working directory.
    base_dir = os.getcwd()
    temp_dir = os.path.join(base_dir, "Temp")
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
        print(f"Directory created: {temp_dir}")
    else:
        print(f"Directory exists: {temp_dir}")
    
    # Set time range in UTC for the last 1 hour.
    end_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
    start_date = (datetime.now(timezone.utc) - timedelta(hours=1)).strftime('%Y-%m-%dT%H:%M:%SZ')
    
    # Get the access token and fetch data from the API.
    auth_token = get_access_token()
    data = fetch_data(start_date, end_date, auth_token)
    
    if data:
        # Convert fetched data into a DataFrame.
        df = pd.DataFrame(data)
        # Save raw data to CSV.
        csv_file_path = save_raw_data_to_csv(df, temp_dir)
        # Clean the CSV (drop "tags", process datetime columns).
        df_clean = clean_csv(csv_file_path)
        
        # Connect to PostgreSQL (adjust connection parameters as needed).
        db_conn = psycopg2.connect(
            dbname="postgres", 
            user="postgres", 
            password="", 
            host="", 
            port=5432
        )
        # Create table if it doesn't exist and insert the cleaned data.
        create_table_and_insert_data(db_conn, "yash", df_clean)
        
        # Delete the CSV file after processing.
        delete_csv(csv_file_path)
        db_conn.close()
    else:
        print("No data fetched for the given time range.")
