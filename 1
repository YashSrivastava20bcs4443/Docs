import requests
import psycopg2
import json
import pandas as pd
from datetime import datetime, timedelta, timezone
import os

# PostgreSQL connection details
DB_CONFIG = {
    "host": "172.16.130.247",
    "database": "postgres",
    "user": "postgres",
    "password": "Zxcv@1234"
}

# API and Access Token Details
API_URL = "https://api-c48.nice-incontact.com/incontactapi/services/v31.0/contacts/completed"
ACCESS_TOKEN_URL = "http://ctiintegrationapi.operations.fareportal.com.local/api/Agent/accessToken"
CSV_FILE_PATH = "completed_contacts.csv"

# Function to generate access token
def get_access_token():
    try:
        response = requests.post(ACCESS_TOKEN_URL)
        response_data = response.json()
        if "access_token" in response_data:
            print("Access Token generated successfully.")
            return response_data['access_token']
        else:
            raise Exception("Failed to generate access token.")
    except Exception as e:
        print(f"Error generating access token: {e}")
        exit()

# Function to fetch data from the API
def fetch_data(start_date, end_date, auth_token):
    headers = {
        "Authorization": f"Bearer {auth_token}",
        "Content-Type": "application/json"
    }
    all_data = []
    skip = 0
    top = 10000  # Fetching maximum 10,000 records per request

    while True:
        params = {
            "startdate": start_date,
            "enddate": end_date,
            "top": top,
            "skip": skip
        }
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()
        # Check if data exists
        if "completedContacts" in data and data["completedContacts"]:
            all_data.extend(data["completedContacts"])
            skip += top
            if len(data["completedContacts"]) < top:
                break
        else:
            break

    print(f"Total records fetched: {len(all_data)}")
    return all_data

# Function to filter data based on specified conditions
def filter_data(data):
    df = pd.DataFrame(data)

    # Debug: Print columns to verify their existence
    print("Columns in the data:", df.columns)

    # Required columns for filtering
    required_columns = [
        'mediaType', 'isOutbound', 'preQueueSeconds', 'inQueueSeconds',
        'agentSeconds', 'masterContactId', 'contactId', 'abandoned', 'endReason'
    ]

    # Check if all required columns exist
    for col in required_columns:
        if col not in df.columns:
            print(f"Column '{col}' not found in the data!")
            return pd.DataFrame()  # Return empty DataFrame if columns are missing

    # Convert data types for filtering
    df['mediaType'] = df['mediaType'].astype(str)
    df['isOutbound'] = df['isOutbound'].astype(bool)
    df['preQueueSeconds'] = pd.to_numeric(df['preQueueSeconds'], errors='coerce').fillna(0)
    df['inQueueSeconds'] = pd.to_numeric(df['inQueueSeconds'], errors='coerce').fillna(0)
    df['agentSeconds'] = pd.to_numeric(df['agentSeconds'], errors='coerce').fillna(0)

    # Apply filtering based on the requirements
    filtered_df = df[
        (df['mediaType'] == '4') &
        (df['isOutbound'] == False) &
        (df['masterContactId'] == df['contactId']) &
        (df['abandoned'] == False) &
        (df['agentSeconds'] == 0) &
        (df['inQueueSeconds'] == 0) &
        (df['preQueueSeconds'] > 1) &
        (df['endReason'].isin(["Contact Hung Up", "Contact Hang Up via Script"]))
    ]

    print(f"Filtered records: {len(filtered_df)}")
    return filtered_df

# Function to save filtered data to a CSV
def save_to_csv(data):
    try:
        data.to_csv(CSV_FILE_PATH, index=False)
        print(f"Data saved to {CSV_FILE_PATH}")
    except Exception as e:
        print(f"Error saving data to CSV: {e}")

# Function to ensure the required columns exist in the PostgreSQL table
def ensure_columns_exist():
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    # Define required columns and data types
    required_columns = {
        "contact_id": "VARCHAR(50)",
        "start_date": "TIMESTAMP",
        "end_date": "TIMESTAMP",
        "from_addr": "VARCHAR(100)",
        "to_addr": "VARCHAR(100)",
        "media_type": "VARCHAR(50)",
        "is_outbound": "BOOLEAN",
        "master_contact_id": "VARCHAR(50)"
    }
    
    # Fetch existing columns
    cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'completed_contacts'")
    existing_columns = {row[0] for row in cursor.fetchall()}
    
    # Add missing columns
    for column, data_type in required_columns.items():
        if column not in existing_columns:
            cursor.execute(f"ALTER TABLE completed_contacts ADD COLUMN {column} {data_type};")
            print(f"Added missing column: {column}")

    conn.commit()
    cursor.close()
    conn.close()

# Function to insert data into PostgreSQL and delete the CSV after insertion
def store_data_to_postgresql_from_csv():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        # Ensure columns exist
        ensure_columns_exist()

        # Load data from CSV
        df = pd.read_csv(CSV_FILE_PATH)

        # Insert data row by row
        for _, row in df.iterrows():
            cursor.execute("""
            INSERT INTO completed_contacts (
                contact_id, start_date, end_date, from_addr, to_addr, media_type, is_outbound, master_contact_id
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                row.get('contactId'), row.get('contactStartDate'), row.get('lastUpdateTime'),
                row.get('fromAddress'), row.get('toAddress'), row.get('mediaType'),
                row.get('isOutbound'), row.get('masterContactId')
            ))

        conn.commit()
        cursor.close()
        conn.close()
        print("Data successfully stored in PostgreSQL.")

        # Delete CSV after successful insertion
        os.remove(CSV_FILE_PATH)
        print(f"{CSV_FILE_PATH} deleted successfully.")

    except Exception as e:
        print(f"Error storing data in PostgreSQL: {e}")

# Main Execution Flow
if __name__ == "__main__":
    # Get the access token
    auth_token = get_access_token()

    # Set the time range for the last 2 hours
    end_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
    start_date = (datetime.now(timezone.utc) - timedelta(hours=2)).strftime('%Y-%m-%dT%H:%M:%SZ')

    # Fetch data from the API
    data = fetch_data(start_date, end_date, auth_token)

    # Filter data and save to CSV
    if data:
        filtered_data = filter_data(data)
        if not filtered_data.empty:
            save_to_csv(filtered_data)
            # Store data in PostgreSQL and delete CSV
            store_data_to_postgresql_from_csv()
        else:
            print("No matching data to save.")
    else:
        print("No data fetched from the API.")
