import streamlit as st
import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
import pytz
import plotly.express as px

# API endpoints
API_URL = "https://api-c48.nice-incontact.com/incontactapi/services/v31.0/contacts/completed"
ACCESS_TOKEN_URL = "http://ctiintegrationapi.operations.fareportal.com.local/api/Agent/accessToken"

# Function to get access token
def get_access_token():
    response = requests.post(ACCESS_TOKEN_URL)
    response_data = response.json()
    if "access token" in response_data:
        return response_data['access token']
    raise Exception("Failed to generate access token")

# Function to fetch data from the API
def fetch_data(start_date, end_date, auth_token):
    headers = {
        "Authorization": f"Bearer {auth_token}",
        "Content-Type": "application/json"
    }
    all_data = []
    skip = 0
    top = 10000
    while True:
        params = {
            "startdate": start_date,
            "enddate": end_date,
            "top": top,
            "skip": skip
        }
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()
        if "completedContacts" in data:
            contacts = data["completedContacts"]
            all_data.extend(contacts)
            skip += top
            if len(contacts) < top:
                break
        else:
            break
    return all_data

# Function to generate a summary based on contact data
def generate_summary(df):
    df['contactStartDate'] = pd.to_datetime(df['contactStartDate'])
    df['contactStartDate'] = df['contactStartDate'].dt.tz_convert('US/Eastern')
    
    summary_df = df.groupby('toAddress').agg(
        offered_calls=('contactId', 'count'),
        answered=('agentSeconds', lambda x: (x > 0).sum()),
        ivr_abandon=('preQueueSeconds', lambda x: ((x > 0) & (df['inQueueSeconds'] == 0)).sum()),
        polite_disconnect=('endReason', lambda x: ((x == 'Contact Hung Up') | (x == 'Contact Hang Up via Script')).sum()),
        queue_abandon=('inQueueSeconds', lambda x: (x > 0).sum()),
    ).reset_index()
    
    return summary_df

# Function to generate IVR bucket summary
def generate_ivr_bucket_summary(df):
    df['contactStartDate'] = pd.to_datetime(df['contactStartDate'])
    df['contactStartDate'] = df['contactStartDate'].dt.tz_convert('US/Eastern')
    df['preQueueSeconds'] = pd.to_numeric(df['preQueueSeconds'], errors='coerce').fillna(0)
    df['time_interval'] = df['contactStartDate'].dt.floor('30min')
    
    # Create IVR bucket based on preQueueSeconds
    df['ivr_bucket'] = pd.cut(
        df['preQueueSeconds'],
        bins=[0, 30, 60, 120, float('inf')],
        labels=['0-30s', '30-60s', '60-120s', '>120s'],
        right=False
    )
    
    # Grouping by time_interval for offered calls and IVR abandon counts
    summary = df.groupby('time_interval').agg(
        Offered_Calls=('contactStartDate', 'count'),
        IVR_Abandon=('ivr_bucket', lambda x: (x.notna()).sum())
    ).reset_index()
    
    # Pivot table for bucket counts
    bucket_counts = df.pivot_table(
        index='time_interval',
        columns='ivr_bucket',
        values='contactStartDate',
        aggfunc='count',
        fill_value=0
    ).reset_index()
    
    # Merge summaries
    final_summary = pd.merge(summary, bucket_counts, on='time_interval', how='left')
    final_summary.columns = ['Interval', 'Offered Calls', 'IVR Abandon', '0-30s', '30-60s', '60-120s', '>120s']
    
    return final_summary

# ----------------- Streamlit App -----------------
st.title("Live Dashboard for Completed Contacts")

# Get the access token and define time range (last 1 hour)
try:
    auth_token = get_access_token()
except Exception as e:
    st.error(f"Error getting auth token: {e}")
    st.stop()

eastern = pytz.timezone('US/Eastern')
end_dt = datetime.now(timezone.utc).astimezone(eastern)
start_dt = end_dt - timedelta(hours=1)

end_date = end_dt.strftime('%Y-%m-%dT%H:%M:%SZ')
start_date = start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# Fetch data
data = fetch_data(start_date, end_date, auth_token)

if data:
    df = pd.DataFrame(data)
    
    st.write("### Raw Data")
    st.dataframe(df)  # Original raw data view

    # ----------------- Real-Time Contact Center Dashboard -----------------
    st.write("### Real-Time Contact Center Dashboard")
    
    # Mapping raw API columns to user-friendly names
    col_mapping = {
        'abandoned': 'Abandoned',
        'abandonSeconds': 'Abandon Seconds',
        'acwseconds': 'ACW Seconds',
        'agentid': 'Agent ID',
        'agentSeconds': 'Agent Seconds',
        'analytics': 'Analytics',
        'ProcessedDate': 'Processed Date',
        'callbackTime': 'Callback Time',
        'campaignid': 'Campaign ID',
        'campaignName': 'Campaign Name',
        'conferenceSeconds': 'Conference Seconds',
        'contactId': 'Contact ID',
        'contactStartDate': 'Contact Start Date',
        'dateACWarehoused': 'Date AC Warehoused',
        'dateContactWarehoused': 'Date Contact Warehoused',
        'dispositionNotes': 'Disposition Notes',
        'endReason': 'End Reason',
        'firstName': 'First Name',
        'fromAddress': 'From Address',
        'highProficiency': 'High Proficiency',
        'holdCount': 'Hold Count',
        'holdSeconds': 'Hold Seconds',
        'inQueueSeconds': 'In Queue Seconds',
        'isAnalytics Processed': 'Is Analytics Processed',
        'isLogged': 'Is Logged',
        'isOutbound': 'Is Outbound',
        'isRefused': 'Is Refused',
        'isShortAbandon': 'Is Short Abandon',
        'isTakeover': 'Is Takeover',
        'lastName': 'Last Name',
        'lastUpdateTime': 'Last Update Time',
        'lowProficiency': 'Low Proficiency',
        'MasterContactId': 'Master Contact ID',
        'mediaSubTypeId': 'Media SubType ID',
        'mediaSubTypeflame': 'Media SubType Flame',
        'mediaTypeId': 'Media Type ID',
        'refuseTime': 'Refuse Time',
        'mediaTypeNane': 'Media Type Name',
        'releaseSeconds': 'Release Seconds',
        'pointOfContactid': 'Point of Contact ID',
        'pointOfContactName': 'Point of Contact Name',
        'postQueueSeconds': 'Post Queue Seconds',
        'preQueueSeconds': 'Pre Queue Seconds',
        'primaryDispositionId': 'Primary Disposition ID',
        'routing Time': 'Routing Time',
        'secondaryDispositionId': 'Secondary Disposition ID',
        'serviceLevelFlag': 'Service Level Flag',
        'skillid': 'Skill ID',
        'skillName': 'Skill Name',
        'transferIndicatorId': 'Transfer Indicator ID',
        'transfer': 'Transfer',
        'IndicatorName': 'Indicator Name'
    }
    
    # Rename columns using the mapping (only if they exist in the data)
    dashboard_df = df.rename(columns=col_mapping)
    
    # Define an ordered list of columns for logical grouping.
    # (Only the columns available in dashboard_df are selected.)
    ordered_dashboard_columns = [
        "Contact Start Date",
        "Processed Date",
        "Callback Time",
        "Last Update Time",
        "Date AC Warehoused",
        "Date Contact Warehoused",
        "Contact ID",
        "Master Contact ID",
        "Campaign ID",
        "Campaign Name",
        "Agent ID",
        "First Name",
        "Last Name",
        "From Address",
        "Media Type ID",
        "Media Type Name",
        "Media SubType ID",
        "Media SubType Flame",
        "Pre Queue Seconds",
        "In Queue Seconds",
        "Post Queue Seconds",
        "Abandon Seconds",
        "ACW Seconds",
        "Agent Seconds",
        "Conference Seconds",
        "Hold Count",
        "Hold Seconds",
        "Release Seconds",
        "Refuse Time",
        "Routing Time",
        "End Reason",
        "Disposition Notes",
        "Primary Disposition ID",
        "Secondary Disposition ID",
        "Service Level Flag",
        "Abandoned",
        "Is Analytics Processed",
        "Analytics",
        "Is Logged",
        "Is Outbound",
        "Is Refused",
        "Is Short Abandon",
        "Is Takeover",
        "High Proficiency",
        "Low Proficiency",
        "Point of Contact ID",
        "Point of Contact Name",
        "Skill ID",
        "Skill Name",
        "Transfer Indicator ID",
        "Transfer",
        "Indicator Name"
    ]
    
    available_columns = [col for col in ordered_dashboard_columns if col in dashboard_df.columns]
    dashboard_df = dashboard_df[available_columns]
    
    st.dataframe(dashboard_df)
    
    # ----------------- Additional Summaries -----------------
    summary_df = generate_summary(df)
    st.write("### TFN-Wise Summary Data")
    st.dataframe(summary_df.style.background_gradient(cmap='viridis'))
    
    ivr_bucket_summary_df = generate_ivr_bucket_summary(df)
    st.write("### IVR Abandon Bucket Wise Data")
    st.dataframe(ivr_bucket_summary_df.style.background_gradient(cmap='plasma'))
else:
    st.write("No data fetched for the given time range.")

