import requests
import pandas as pd
import os
import psycopg2
from psycopg2 import sql
from datetime import datetime, timedelta, timezone
import pytz
import json

# ================================================
# Configuration & Constants
# ================================================


# Timezone conversion: UTC to EST
EST_TZ = pytz.timezone("US/Eastern")

# List of datetime columns to be processed
DATETIME_COLUMNS = [
    "contactStartDate", 
    "dateACWWarehoused", 
    "dateContactWarehoused", 
    "lastUpdateTime"
]

# Define directories: BASE_DIR is where the script runs; TEMP_DIR is inside BASE_DIR.
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(BASE_DIR, "Temp")
os.makedirs(TEMP_DIR, exist_ok=True)

# Mapping file path (should be in the same folder as this script)
MAPPING_FILE = os.path.join(BASE_DIR, "mapping.json")


# ================================================
# Functions
# ================================================

def get_access_token():
    """
    Fetch the access token from the API.
    """
    response = requests.post(ACCESS_TOKEN_URL)
    response_data = response.json()
    if "access_token" in response_data:
        print("Access Token generated successfully.")
        return response_data["access_token"]
    else:
        raise Exception("Failed to generate access token")


def fetch_data(start_date, end_date, auth_token):
    """
    Fetch data from the API within the given time range.
    Pagination is handled using 'top' and 'skip' parameters.
    """
    headers = {"Authorization": f"Bearer {auth_token}", "Content-Type": "application/json"}
    all_data = []
    top = 10000
    skip = 0
    while True:
        params = {"startDate": start_date, "endDate": end_date, "top": top, "skip": skip}
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()
        if "completedContacts" in data and data["completedContacts"]:
            all_data.extend(data["completedContacts"])
            skip += top
            if len(data["completedContacts"]) < top:
                break
        else:
            break
    print(f"Total records fetched: {len(all_data)}")
    return all_data


def save_raw_data_to_csv(data):
    """
    Clean the data:
      - Convert datetime columns from UTC to EST.
      - Remove the 'T' and 'Z' characters.
      - Drop the 'tags' column if it exists.
      - Load mapping.json and add new columns (Description, TFN Type, Portal).
        For each row, if the CSV's 'toAddress' matches the mapping's 'Number',
        the corresponding data is filled; otherwise, "Other" is filled.
    Save the cleaned data as a CSV in the Temp directory.
    """
    df = pd.DataFrame(data)
    
    # Process datetime columns: convert from UTC to EST and format accordingly
    for col in DATETIME_COLUMNS:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce", utc=True)
            df[col] = df[col].dt.tz_convert(EST_TZ)
            df[col] = df[col].dt.strftime("%Y-%m-%d %H:%M:%S")
    
    # Drop 'tags' column if present
    if "tags" in df.columns:
        df.drop(columns=["tags"], inplace=True)
        print("Dropped 'tags' column from CSV.")
    
    # Load mapping data from mapping.json
    try:
        with open(MAPPING_FILE, 'r') as f:
            mapping_data = json.load(f)
    except Exception as e:
        print(f"Error reading mapping.json: {e}")
        mapping_data = []
    
    # Add new columns: Description, TFN Type, Portal
    df["Description"] = ""
    df["TFN Type"] = ""
    df["Portal"] = ""
    
    # For each row, match 'toAddress' with mapping 'Number'
    if "toAddress" in df.columns:
        for index, row in df.iterrows():
            to_address = str(row["toAddress"]).strip()
            found_match = False
            for mapping in mapping_data:
                if str(mapping.get("Number", "")).strip() == to_address:
                    df.at[index, "Description"] = mapping.get("Description", "")
                    df.at[index, "TFN Type"] = mapping.get("TFN Type", "")
                    df.at[index, "Portal"] = mapping.get("Portal", "")
                    found_match = True
                    break  # Stop after finding the first match
            if not found_match:
                df.at[index, "Description"] = "Other"
                df.at[index, "TFN Type"] = "Other"
                df.at[index, "Portal"] = "Other"
    else:
        print("Warning: 'toAddress' column not found; mapping cannot be applied.")
    
    # Save the cleaned DataFrame as CSV in the Temp directory
    csv_file_path = os.path.join(TEMP_DIR, "Raw_data.csv")
    df.to_csv(csv_file_path, index=False)
    print(f"CSV file cleaned and updated at '{csv_file_path}'")
    return csv_file_path


def create_table_and_insert_data(db_conn, table_name, csv_file_path):
    """
    Read the CSV file, create the table (if not exists) with all columns as TEXT,
    and insert the CSV data into the table.
    """
    # Load CSV data into DataFrame
    df = pd.read_csv(csv_file_path)
    # Replace missing/NaN values with None for SQL compatibility
    df = df.where(pd.notnull(df), None)
    
    cursor = db_conn.cursor()
    
    # Get column names and define all columns as TEXT
    columns = df.columns
    column_definitions = ", ".join([f'"{col}" TEXT' for col in columns])
    create_table_query = f'CREATE TABLE IF NOT EXISTS "{table_name}" ({column_definitions});'
    cursor.execute(create_table_query)
    db_conn.commit()
    print(f"Table '{table_name}' created (if it did not already exist).")
    
    # Build INSERT query (all values are inserted as TEXT)
    insert_query = sql.SQL(
        "INSERT INTO {} ({}) VALUES ({})"
    ).format(
        sql.Identifier(table_name),
        sql.SQL(", ").join(map(sql.Identifier, columns)),
        sql.SQL(", ").join(sql.Placeholder() * len(columns))
    )
    
    # Insert each row from the DataFrame
    for row in df.itertuples(index=False, name=None):
        # Convert each value to a string if not None
        row_converted = [str(val) if val is not None else None for val in row]
        cursor.execute(insert_query, row_converted)
    
    db_conn.commit()
    cursor.close()
    print(f"Data inserted into table '{table_name}' successfully!")


def delete_csv(file_path):
    """
    Delete the specified CSV file.
    """
    os.remove(file_path)
    print(f"CSV file '{file_path}' deleted successfully!")


# ================================================
# Main Execution
# ================================================
if __name__ == "__main__":
    try:
        # 1️⃣ Get Access Token
        auth_token = get_access_token()
        
        # Define time range: last 1 hour (UTC)
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(hours=1)
        end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')
        start_date_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        # 2️⃣ Fetch data from API
        data = fetch_data(start_date_str, end_date_str, auth_token)
        
        if data:
            # 3️⃣ Clean data, drop 'tags', apply mapping, and save CSV in Temp directory
            csv_file_path = save_raw_data_to_csv(data)
            
            # 4️⃣ Connect to PostgreSQL, create table (if needed) and insert data from CSV
            db_conn = psycopg2.connect(**DB_CONFIG)
            create_table_and_insert_data(db_conn, "yash", csv_file_path)
            
            # 5️⃣ Delete the CSV file after successful insertion
            delete_csv(csv_file_path)
            db_conn.close()
        else:
            print("No data fetched for the given time range.")
    except Exception as e:
        print("Error:", e)

