def push_data_to_common_events(data):
    message = data  # Since Splunk raw alert already comes as dict
    event_blob = json.dumps(message)  # Store full alert JSON
    event_source = message.get("detector", "Splunk")
    event_node = message.get("dimensions", {}).get("location", "Unknown")
    event_instance = message.get("dimensions", {}).get("test", "")
    event_value = message.get("inputs", {}).get("stream", {}).get("value", "")
    event_resource = message.get("originatingMetric", "Unknown")
    event_title = message.get("messageTitle", "Splunk Alert")
    event_level = message.get("severity", "CRITICAL").upper()
    event_timestamp = message.get("timestamp", data.get("@timestamp", utc_now))

    concatenated_values = f"{event_source}{event_node}{event_resource}{event_instance}"
    eventmatchid = hashlib.sha256(concatenated_values.encode()).hexdigest()

    mapped_data = {
        "EventLevel": event_level,
        "EventStatus": "Active",
        "EventTitle": event_title,
        "EventBlob": event_blob,
        "EventTimestamp": event_timestamp,
        "@timestamp": utc_now,
        "EventSource": event_source,
        "EventNode": event_node,
        "EventInstance": event_instance,
        "EventValue": event_value,
        "EventResource": event_resource,
        "EventMatchID": eventmatchid
    }

    # Print mapped data for testing
    print("\n--- Mapped Data to be Indexed in Common Index ---")
    for k, v in mapped_data.items():
        print(f"{k}: {v}")
    print("--------------------------------------------------")

    # Uncomment the line below when ready to index into Elasticsearch
    # response = es.index(index=common_index_name, body=mapped_data)
