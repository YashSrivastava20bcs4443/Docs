import requests
import json
import pandas as pd
import psycopg2
from datetime import datetime, timedelta, timezone
import os

# API and Access Token Details
API_URL = "https://api-c48.nice-incontact.com/incontactapi/services/v31.0/contacts/completed"
ACCESS_TOKEN_URL = "http://ctiintegrationapi.operations.fareportal.com.local/api/Agent/accessToken"

# PostgreSQL connection details
DB_CONFIG = {
    "host": "172.16.130.247",
    "database": "postgres",
    "user": "postgres",
    "password": "Zxcv@1234"
}

# Generate Access Token
def get_access_token():
    try:
        response = requests.post(ACCESS_TOKEN_URL)
        response_data = response.json()
        if "access_token" in response_data:
            print("Access Token generated successfully.")
            return response_data['access_token']
        else:
            raise Exception("Failed to generate access token.")
    except Exception as e:
        print(f"Error generating access token: {e}")
        exit()

# Fetch data from API
def fetch_data(start_date, end_date, auth_token):
    headers = {
        "Authorization": f"Bearer {auth_token}",
        "Content-Type": "application/json"
    }
    all_data = []
    skip = 0
    top = 10000  

    while True:
        params = {
            "startdate": start_date,
            "enddate": end_date,
            "top": top,
            "skip": skip
        }
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()

        if "completedContacts" in data and data["completedContacts"]:
            all_data.extend(data["completedContacts"])
            skip += top
            if len(data["completedContacts"]) < top:
                break
        else:
            break

    print(f"Total records fetched: {len(all_data)}")
    return all_data

# Store raw data in CSV
def store_raw_data(data):
    df = pd.DataFrame(data)
    df.to_csv('raw_data.csv', index=False)
    print("Raw data stored in raw_data.csv")

# Filter data based on requirements
def filter_data(df):
    # Convert columns to appropriate types
    df['agentSeconds'] = pd.to_numeric(df['agentSeconds'], errors='coerce').fillna(0).astype('Int64')
    df['inQueueSeconds'] = pd.to_numeric(df['inQueueSeconds'], errors='coerce').fillna(0).astype('Int64')
    df['preQueueSeconds'] = pd.to_numeric(df['preQueueSeconds'], errors='coerce').fillna(0).astype('Int64')
    df['totalDurationSeconds'] = pd.to_numeric(df['totalDurationSeconds'], errors='coerce').fillna(0).astype('Int64')

    # Filter based on the provided conditions
    filtered_data = df[
        (df['mediaTypeId'] == 4) &
        (df['isOutbound'] == False) &
        (df['masterContactId'] == df['contactId'])
    ]

    return filtered_data

# Push data to PostgreSQL
def push_data_to_db(df):
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        # Create table if not exists
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS raw_contacts (
                contactId BIGINT,
                masterContactId BIGINT,
                agentSeconds BIGINT,
                inQueueSeconds BIGINT,
                preQueueSeconds BIGINT,
                totalDurationSeconds BIGINT,
                abandoned BOOLEAN,
                endReason VARCHAR,
                toAddress VARCHAR,
                contactStartDate TIMESTAMP,
                lastUpdateTime TIMESTAMP
            )
        """)
        conn.commit()

        # Insert data into the table
        for index, row in df.iterrows():
            cursor.execute("""
                INSERT INTO raw_contacts (contactId, masterContactId, agentSeconds, inQueueSeconds, preQueueSeconds, totalDurationSeconds, abandoned, endReason, toAddress, contactStartDate, lastUpdateTime)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                row['contactId'],
                row['masterContactId'],
                row['agentSeconds'],
                row['inQueueSeconds'],
                row['preQueueSeconds'],
                row['totalDurationSeconds'],
                row['abandoned'],
                row['endReason'],
                row['toAddress'],
                row['contactStartDate'],
 row['lastUpdateTime']
            ))
        conn.commit()
        print("Data pushed to PostgreSQL successfully.")
    except Exception as e:
        print(f"Error pushing data to DB: {e}")
    finally:
        cursor.close()
        conn.close()

# Create expected data table
def create_expected_data_table():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        # Create expected data table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS expected_data (
                toAddress VARCHAR PRIMARY KEY,
                offeredCalls INT,
                answered INT,
                ivrAbandon INT,
                queueAbandon INT,
                politeDisconnect INT
            )
        """)
        conn.commit()
    except Exception as e:
        print(f"Error creating expected data table: {e}")
    finally:
        cursor.close()
        conn.close()

# Delete CSV file
def delete_csv_file():
    if os.path.exists('raw_data.csv'):
        os.remove('raw_data.csv')
        print("CSV file deleted.")

# Main Execution Flow
if __name__ == "__main__":
    # Generate Access Token
    auth_token = get_access_token()

    # Set the time range for the last 2 hours
    end_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
    start_date = (datetime.now(timezone.utc) - timedelta(hours=2)).strftime('%Y-%m-%dT%H:%M:%SZ')

    # Fetch data from the API
    data = fetch_data(start_date, end_date, auth_token)
    if data:
        print("Data fetched successfully.")
        store_raw_data(data)

        # Load raw data from CSV
        df = pd.read_csv('raw_data.csv')

        # Filter data
        filtered_data = filter_data(df)

        # Push filtered data to DB
        push_data_to_db(filtered_data)

        # Create expected data table
        create_expected_data_table()

        # Delete CSV file
        delete_csv_file()
    else:
        print("No data fetched for the given time range.")
