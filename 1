# Step 1: Locate the latest consolidated file
consolidated_files = sorted([f for f in list_sftp_files(sftp, f"{sftp_base_dir}/Linux") if 'consolidated' in f])
if not consolidated_files:
    print("No consolidated files found.")
    exit(1)

latest_consolidated = consolidated_files[-1]
latest_consolidated_path = f"{sftp_base_dir}/Linux/{latest_consolidated}"
local_consolidated_path = os.path.join(local_temp_dir, latest_consolidated)

# Download the latest consolidated file
download_file(sftp, latest_consolidated_path, local_consolidated_path)

# Load the latest consolidated data
consolidated_df = pd.read_csv(local_consolidated_path)

# Step 2: Find new daily files
daily_files = sorted([f for f in list_sftp_files(sftp, sftp_base_dir) if 'Last_24_hr_Linux_Performance_metrics' in f])

# Only consider daily files after the last consolidated file's date
# Modify the date extraction logic
last_consolidated_date_str = latest_consolidated.split('_')[2:5]  # Extract date parts
last_consolidated_date_str = '-'.join(last_consolidated_date_str)  # Combine as 'YYYY-MM-DD'
last_consolidated_date = datetime.strptime(last_consolidated_date_str, '%Y-%m-%d')

# Filter daily files based on the last consolidated date
daily_files_to_process = [f for f in daily_files if datetime.strptime(f.split('_')[-2], '%Y-%m-%d') > last_consolidated_date]

# Step 3: Process each daily file and merge with the consolidated file
for i, daily_file in enumerate(daily_files_to_process):
    daily_file_path = f"{sftp_base_dir}/Linux/{daily_file}"
    local_daily_path = os.path.join(local_temp_dir, daily_file)

    # Download the daily file
    download_file(sftp, daily_file_path, local_daily_path)

    # Load the daily data
    daily_df = pd.read_csv(local_daily_path)

    # Merge with the existing consolidated data
    consolidated_df = merge_max_values(consolidated_df, daily_df)

    # Determine the new consolidated file name
    new_consolidated_name = f"{i + 31}days_consolidated_linux_{daily_file.split('_')[-2]}.csv"
    new_consolidated_path = os.path.join(local_temp_dir, new_consolidated_name)

    # Save the new consolidated data locally
    consolidated_df.to_csv(new_consolidated_path, index=False)

    # Upload the new consolidated file to SFTP
    upload_file(sftp, new_consolidated_path, f"{sftp_base_dir}/Linux/{new_consolidated_name}")

# Cleanup: Delete local temp directory
for file in os.listdir(local_temp_dir):
    os.remove(os.path.join(local_temp_dir, file))
os.rmdir(local_temp_dir)
print("Local temp directory cleaned up.")
