import requests
import json
import csv
import os
import psycopg2
from datetime import datetime, timedelta, timezone

# API and Access Token Details
API_URL = "https://api-c48.nice-incontact.com/incontactapi/services/v31.0/contacts/completed"
ACCESS_TOKEN_URL = "http://ctiintegrationapi.operations.fareportal.com.local/api/Agent/accessToken"

# Database Configuration
DB_CONFIG = {
    "host": "172.16.130.247",
    "database": "postgres",
    "user": "postgres",
    "password": "Zxcv@1234"
}

CSV_FILE = "raw_data.csv"

# Generate Access Token
def get_access_token():
    response = requests.post(ACCESS_TOKEN_URL)
    response_data = response.json()
    if "access_token" in response_data:
        print("Access Token generated successfully.")
        return response_data['access_token']
    else:
        raise Exception("Failed to generate access token.")

# Fetch data from API
def fetch_data(start_date, end_date, auth_token):
    headers = {
        "Authorization": f"Bearer {auth_token}",
        "Content-Type": "application/json"
    }
    all_data = []
    skip = 0
    top = 10000

    while True:
        params = {
            "startdate": start_date,
            "enddate": end_date,
            "top": top,
            "skip": skip
        }
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()

        if "completedContacts" in data and data["completedContacts"]:
            all_data.extend(data["completedContacts"])
            skip += top
            if len(data["completedContacts"]) < top:
                break
        else:
            break

    print(f"Total records fetched: {len(all_data)}")
    return all_data

# Save data to a CSV file
def save_to_csv(data, file_name="data.csv"):
    if not data:
        print("No data to save.")
        return

    fieldnames = set()
    for record in data:
        fieldnames.update(record.keys())
    fieldnames = list(fieldnames)

    try:
        with open(file_name, mode="w", newline="", encoding="utf-8") as file:
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"Data saved successfully to {file_name}.")
    except Exception as e:
        print(f"Error saving to CSV: {e}")

# Create the `raw_df` table if it doesn't exist
def create_raw_df_table(conn):
    query = """
    CREATE TABLE IF NOT EXISTS raw_df (
        abandoned BOOLEAN,
        abandonSeconds FLOAT,
        acwSeconds FLOAT,
        agentId BIGINT,
        agentSeconds FLOAT,
        analyticsProcessedDate TIMESTAMP,
        callbackTime INTEGER,
        campaignId BIGINT,
        campaignName TEXT,
        contactId BIGINT,
        contactStartDate TIMESTAMP,
        dateACWWarehoused TIMESTAMP,
        dateContactWarehoused TIMESTAMP,
        dispositionNotes TEXT,
        endReason TEXT,
        firstName TEXT,
        fromAddress TEXT,
        highProficiency BOOLEAN,
        holdCount INTEGER,
        holdSeconds FLOAT,
        inQueueSeconds FLOAT,
        isAnalyticsProcessed BOOLEAN,
        isLogged BOOLEAN,
        isOutbound BOOLEAN,
        isRefused BOOLEAN,
        isShortAbandon BOOLEAN,
        isTakeover BOOLEAN,
        lastName TEXT,
        lastUpdateTime TIMESTAMP,
        lowProficiency BOOLEAN,
        masterContactId BIGINT,
        mediaSubTypeId INTEGER,
        mediaSubTypeName TEXT,
        mediaTypeId INTEGER,
        mediaTypeName TEXT,
        pointOfContactId BIGINT,
        pointOfContactName TEXT,
        postQueueSeconds FLOAT,
        preQueueSeconds FLOAT,
        primaryDispositionId INTEGER,
        refuseReason TEXT,
        refuseTime FLOAT,
        releaseSeconds FLOAT,
        routingAttribute TEXT,
        routingTime FLOAT,
        secondaryDispositionId INTEGER,
        serviceLevelFlag BOOLEAN,
        skillId BIGINT,
        skillName TEXT,
        teamId BIGINT,
        teamName TEXT,
        toAddress TEXT,
        totalDurationSeconds FLOAT,
        transferIndicatorId INTEGER,
        transferIndicatorName TEXT
    );
    """
    cursor = conn.cursor()
    cursor.execute(query)
    conn.commit()
    cursor.close()

# Load data into PostgreSQL
def load_to_postgres(table_name):
    conn = psycopg2.connect(**DB_CONFIG)
    create_raw_df_table(conn)  # Ensure the table exists
    cursor = conn.cursor()
    with open(CSV_FILE, "r") as f:
        cursor.copy_expert(f"COPY {table_name} FROM STDIN WITH CSV HEADER", f)
    conn.commit()
    print(f"Data loaded into table {table_name}.")
    cursor.close()
    conn.close()

# Filter data and save to a new table
def process_and_store_data():
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()

    query = """
        CREATE TABLE IF NOT EXISTS processed_df AS
        SELECT 
            toaddress AS number,
            COUNT(*) FILTER (WHERE mediatypeid = 4 AND isoutbound = false) AS offered_calls,
            COUNT(*) FILTER (WHERE mediatypeid = 4 AND isoutbound = false AND agentseconds > 0) AS answered,
            COUNT(*) FILTER (WHERE mediatypeid = 4 AND isoutbound = false AND agentseconds = 0 AND inqueueseconds = 0 AND prequeueseconds > 1 AND endreason = 'Contact Hung Up') AS ivr_abandon,
            COUNT(*) FILTER (WHERE mediatypeid = 4 AND isoutbound = true AND agentseconds = 0 AND inqueueseconds > 0 AND prequeueseconds > 0) AS queue_abandon,
            COUNT(*) FILTER (WHERE mediatypeid = 4 AND isoutbound = false AND agentseconds = 0 AND inqueueseconds = 0 AND prequeueseconds > 1 AND endreason = 'Contact Hang Up via Script') AS polite_disconnect
        FROM raw_df
        GROUP BY toaddress;
    """
    cursor.execute(query)
    conn.commit()
    print("Filtered data processed and saved.")
    cursor.close()
    conn.close()

# Main Execution
if __name__ == "__main__":
    auth_token = get_access_token()

    # Fetch data for the last 2 hours
    end_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
    start_date = (datetime.now(timezone.utc) - timedelta(hours=2)).strftime('%Y-%m-%dT%H:%M:%SZ')
    data = fetch_data(start_date, end_date, auth_token)

    if data:
        save_to_csv(data, CSV_FILE)
        load_to_postgres("raw_df")
        process_and_store_data()
        os.remove(CSV_FILE)
        print(f"{CSV_FILE} deleted.")
    else:
        print("No data fetched.")
