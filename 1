import requests
import psycopg2
import json
import pandas as pd
from datetime import datetime, timedelta, timezone
import os

# PostgreSQL connection details
DB_CONFIG = {
    "host": "172.16.130.247",
    "database": "postgres",
    "user": "postgres",
    "password": "Zxcv@1234"
}

# API and Access Token Details
API_URL = "https://api-c48.nice-incontact.com/incontactapi/services/v31.0/contacts/completed"
ACCESS_TOKEN_URL = "http://ctiintegrationapi.operations.fareportal.com.local/api/Agent/accessToken"

# Function to generate access token
def get_access_token():
    try:
        response = requests.post(ACCESS_TOKEN_URL)
        response_data = response.json()
        if "access_token" in response_data:
            print("Access Token generated successfully.")
            return response_data['access_token']
        else:
            raise Exception("Failed to generate access token.")
    except Exception as e:
        print(f"Error generating access token: {e}")
        exit()

# Function to fetch data from the API
def fetch_data(start_date, end_date, auth_token):
    headers = {
        "Authorization": f"Bearer {auth_token}",
        "Content-Type": "application/json"
    }
    all_data = []
    skip = 0
    top = 10000  # Fetching maximum 10,000 records per request

    while True:
        params = {
            "startdate": start_date,
            "enddate": end_date,
            "top": top,
            "skip": skip
        }
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()

        if "completedContacts" in data and data["completedContacts"]:
            all_data.extend(data["completedContacts"])
            skip += top
            if len(data["completedContacts"]) < top:
                break
        else:
            break

    print(f"Total records fetched: {len(all_data)}")
    return all_data

# Save data to CSV
def save_data_to_csv(data):
    try:
        df = pd.DataFrame(data)
        csv_file = "new_completed_contacts.csv"
        df.to_csv(csv_file, index=False)
        print(f"Data saved to {csv_file}")
        return csv_file
    except Exception as e:
        print(f"Error saving data to CSV: {e}")

# Ensure the metrics table exists with the correct columns
def create_metrics_table_if_not_exists():
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    create_table_query = """
    CREATE TABLE IF NOT EXISTS call_metrics (
        toll_number VARCHAR(50),
        offered_calls INT,
        answered INT,
        ivr_abandon INT,
        queue_abandon INT,
        polite_disconnect INT
    );
    """
    cursor.execute(create_table_query)
    conn.commit()
    print("Table 'call_metrics' is ready.")
    cursor.close()
    conn.close()

# Function to filter data based on your conditions and prepare metrics
def filter_and_prepare_metrics(data):
    df = pd.DataFrame(data)

    # Check for column and rename if necessary
    if 'mediaTypeId' in df.columns:
        df.rename(columns={'mediaTypeId': 'mediaType'}, inplace=True)
    elif 'mediaTypeld' in df.columns:
        df.rename(columns={'mediaTypeld': 'mediaType'}, inplace=True)

    # Ensure required columns exist
    required_columns = [
        'toAddress', 'mediaType', 'isOutbound', 'masterContactId', 'contactId',
        'abandoned', 'agentSeconds', 'inQueueSeconds', 'preQueueSeconds', 'endReason'
    ]

    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Missing columns in the dataset: {missing_columns}")
        return pd.DataFrame()

    # Convert columns for proper filtering
    df['mediaType'] = df['mediaType'].astype(str)
    df['isOutbound'] = df['isOutbound'].astype(bool)
    df['preQueueSeconds'] = pd.to_numeric(df['preQueueSeconds'], errors='coerce').fillna(0)
    df['inQueueSeconds'] = pd.to_numeric(df['inQueueSeconds'], errors='coerce').fillna(0)
    df['agentSeconds'] = pd.to_numeric(df['agentSeconds'], errors='coerce').fillna(0)
    df['abandoned'] = df['abandoned'].astype(bool)

    # Generate metrics based on conditions
    metrics = df[df['mediaType'] == '4'].groupby('toAddress').agg(
        offered_calls=('contactId', 'count'),
        answered=('contactId', lambda x: ((df['isOutbound'] == False) & (df['masterContactId'] == df['contactId'])).sum()),
        ivr_abandon=('contactId', lambda x: ((df['abandoned'] == True) & (df['inQueueSeconds'] == 0)).sum()),
        queue_abandon=('contactId', lambda x: ((df['isOutbound'] == True) & (df['abandoned'] == True)).sum()),
        polite_disconnect=('contactId', lambda x: ((df['endReason'] == 'Contact Hung Up') & 
                                                   (df['agentSeconds'] == 0) & 
                                                   (df['preQueueSeconds'] > 1)).sum())
    ).reset_index()

    print(f"Metrics prepared for {len(metrics)} unique toll numbers.")
    return metrics

# Store metrics into PostgreSQL
def store_metrics_to_postgresql(metrics):
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()

        create_metrics_table_if_not_exists()  # Ensure table exists before inserting

        for _, row in metrics.iterrows():
            cursor.execute("""
                INSERT INTO call_metrics (toll_number, offered_calls, answered, ivr_abandon, queue_abandon, polite_disconnect)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                row['toAddress'],
                row['offered_calls'],
                row['answered'],
                row['ivr_abandon'],
                row['queue_abandon'],
                row['polite_disconnect']
            ))

        conn.commit()
        print("Metrics successfully stored in PostgreSQL.")
        cursor.close()
        conn.close()
    except Exception as e:
        print(f"Error storing metrics to PostgreSQL: {e}")

# Delete CSV file after data insertion
def delete_csv_file(csv_file):
    try:
        os.remove(csv_file)
        print(f"{csv_file} deleted successfully.")
    except Exception as e:
        print(f"Error deleting CSV file: {e}")

# Main Execution Flow
if __name__ == "__main__":
    # Get the access token
    auth_token = get_access_token()

    # Set the time range for the last 2 hours
    end_date = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
    start_date = (datetime.now(timezone.utc) - timedelta(hours=2)).strftime('%Y-%m-%dT%H:%M:%SZ')

    # Fetch data from the API
    data = fetch_data(start_date, end_date, auth_token)

    if data:
        # Save data to CSV and filter it for metrics generation
        csv_file = save_data_to_csv(data)
        metrics_data = filter_and_prepare_metrics(data)

        # Store metrics in PostgreSQL if data is available
        if not metrics_data.empty:
            store_metrics_to_postgresql(metrics_data)
            delete_csv_file(csv_file)
        else:
            print("No matching records found for metrics calculation.")
    else:
        print("No data fetched for the given time range.")
