import streamlit as st
import requests
import pandas as pd
from datetime import datetime, timedelta, timezone
import pytz
import plotly.express as px
from streamlit_plotly_events import plotly_events  # For capturing click events

# ----------------- API and Data Fetching Functions -----------------
API_URL = "https://api-c48.nice-incontact.com/incontactapi/services/v31.0/contacts/completed"
ACCESS_TOKEN_URL = "http://ctiintegrationapi.operations.fareportal.com.local/api/Agent/accessToken"

def get_access_token():
    response = requests.post(ACCESS_TOKEN_URL)
    response_data = response.json()
    if "access token" in response_data:
        return response_data['access token']
    raise Exception("Failed to generate access token")

def fetch_data(start_date, end_date, auth_token):
    headers = {
        "Authorization": f"Bearer {auth_token}",
        "Content-Type": "application/json"
    }
    all_data = []
    skip = 0
    top = 10000
    while True:
        params = {"startdate": start_date, "enddate": end_date, "top": top, "skip": skip}
        response = requests.get(API_URL, headers=headers, params=params)
        data = response.json()
        if "completedContacts" in data:
            contacts = data["completedContacts"]
            all_data.extend(contacts)
            skip += top
            if len(contacts) < top:
                break
        else:
            break
    return all_data

# ----------------- Data Transformation & Summary Functions -----------------
def generate_summary(df):
    # Adjust date and time zone
    df['contactStartDate'] = pd.to_datetime(df['contactStartDate'], errors='coerce')
    df['contactStartDate'] = df['contactStartDate'].dt.tz_convert('US/Eastern')
    summary_df = df.groupby('toAddress').agg(
        offered_calls=('contactId', 'count'),
        answered=('agentSeconds', lambda x: (x > 0).sum()),
        ivr_abandon=('preQueueSeconds', lambda x: ((x > 0) & (df['inQueueSeconds'] == 0)).sum()),
        polite_disconnect=('endReason', lambda x: ((x == 'Contact Hung Up') | (x == 'Contact Hang Up via Script')).sum()),
        queue_abandon=('inQueueSeconds', lambda x: (x > 0).sum()),
    ).reset_index()
    return summary_df

def generate_ivr_bucket_summary(df):
    df['contactStartDate'] = pd.to_datetime(df['contactStartDate'], errors='coerce')
    df['contactStartDate'] = df['contactStartDate'].dt.tz_convert('US/Eastern')
    df['preQueueSeconds'] = pd.to_numeric(df['preQueueSeconds'], errors='coerce').fillna(0)
    df['time_interval'] = df['contactStartDate'].dt.floor('30min')
    df['ivr_bucket'] = pd.cut(
        df['preQueueSeconds'],
        bins=[0, 30, 60, 120, float('inf')],
        labels=['0-30s', '30-60s', '60-120s', '>120s'],
        right=False
    )
    summary = df.groupby('time_interval').agg(
        Offered_Calls=('contactStartDate', 'count'),
        IVR_Abandon=('ivr_bucket', lambda x: (x.notna()).sum())
    ).reset_index()
    bucket_counts = df.pivot_table(
        index='time_interval',
        columns='ivr_bucket',
        values='contactStartDate',
        aggfunc='count',
        fill_value=0
    ).reset_index()
    final_summary = pd.merge(summary, bucket_counts, on='time_interval', how='left')
    final_summary.columns = ['Interval', 'Offered Calls', 'IVR Abandon', '0-30s', '30-60s', '60-120s', '>120s']
    return final_summary

def generate_kpi_metrics(df):
    numeric_cols = ['agentSeconds', 'holdSeconds', 'inQueueSeconds', 'abandonSeconds', 'preQueueSeconds']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    total_calls = len(df)
    answered_calls = df[df['agentSeconds'] > 0].shape[0] if 'agentSeconds' in df.columns else 0
    if 'abandoned' in df.columns:
        df['abandoned_bool'] = df['abandoned'].astype(bool)
        abandoned_calls = df['abandoned_bool'].sum()
    else:
        abandoned_calls = df[(df['preQueueSeconds'] > 0) & (df['inQueueSeconds'] == 0)].shape[0]
    abandon_rate = (abandoned_calls / total_calls * 100) if total_calls > 0 else 0
    avg_agent_time = df['agentSeconds'].mean() if 'agentSeconds' in df.columns else 0
    avg_hold_time = df['holdSeconds'].mean() if 'holdSeconds' in df.columns else 0
    avg_wait_time = df['inQueueSeconds'].mean() if 'inQueueSeconds' in df.columns else 0
    avg_call_duration = avg_agent_time  # Using agentSeconds as proxy
    return {
        "Total Calls": total_calls,
        "Answered Calls": answered_calls,
        "Abandoned Calls": abandoned_calls,
        "Abandon Rate (%)": round(abandon_rate, 2),
        "Avg Agent Time (sec)": round(avg_agent_time, 2),
        "Avg Hold Time (sec)": round(avg_hold_time, 2),
        "Avg Wait Time (sec)": round(avg_wait_time, 2),
        "Avg Call Duration (sec)": round(avg_call_duration, 2)
    }

def generate_time_series(df):
    if 'contactStartDate' in df.columns:
        df['contactStartDate'] = pd.to_datetime(df['contactStartDate'], errors='coerce')
        df['minute_interval'] = df['contactStartDate'].dt.floor('T')
        ts = df.groupby('minute_interval').size().reset_index(name='Calls')
        return ts
    else:
        return pd.DataFrame(columns=['minute_interval', 'Calls'])

def generate_campaign_distribution(df):
    if 'campaignName' in df.columns:
        camp = df.groupby('campaignName').size().reset_index(name='Calls')
        return camp
    else:
        return pd.DataFrame(columns=['campaignName', 'Calls'])

def generate_end_reason_distribution(df):
    if 'endReason' in df.columns:
        er = df.groupby('endReason').size().reset_index(name='Calls')
        return er
    else:
        return pd.DataFrame(columns=['endReason', 'Calls'])

def generate_top_agents(df, top_n=5):
    if "Agent ID" in df.columns:
        top_agents = df.groupby("Agent ID").size().reset_index(name="Calls")
        top_agents = top_agents.sort_values("Calls", ascending=False).head(top_n)
        return top_agents
    return pd.DataFrame(columns=["Agent ID", "Calls"])

# ----------------- Main App Layout -----------------
st.title("niceLiveUCX Dashboard")

# --- Get token and set time range (last 1 hour) ---
try:
    auth_token = get_access_token()
except Exception as e:
    st.error(f"Error getting auth token: {e}")
    st.stop()

eastern = pytz.timezone('US/Eastern')
end_dt = datetime.now(timezone.utc).astimezone(eastern)
start_dt = end_dt - timedelta(hours=1)

# For API calls, keep the ISO format (with T and Z)
end_date = end_dt.strftime('%Y-%m-%dT%H:%M:%SZ')
start_date = start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')

# --- Fetch Data ---
data = fetch_data(start_date, end_date, auth_token)
if not data:
    st.write("No data fetched for the given time range.")
    st.stop()

df = pd.DataFrame(data)

# --- Raw Data (for reference) ---
st.write("### Raw Data")
st.dataframe(df)

# --- Rename Columns for Dashboard ---
col_mapping = {
    'abandoned': 'Abandoned',
    'abandonSeconds': 'Abandon Seconds',
    'acwseconds': 'ACW Seconds',
    'agentid': 'Agent ID',
    'agentSeconds': 'Agent Seconds',
    'analytics': 'Analytics',
    'ProcessedDate': 'Processed Date',
    'callbackTime': 'Callback Time',
    'campaignid': 'Campaign ID',
    'campaignName': 'Campaign Name',
    'conferenceSeconds': 'Conference Seconds',
    'contactId': 'Contact ID',
    'contactStartDate': 'Contact Start Date',
    'dateACWarehoused': 'Date AC Warehoused',
    'dateContactWarehoused': 'Date Contact Warehoused',
    'dispositionNotes': 'Disposition Notes',
    'endReason': 'End Reason',
    'firstName': 'First Name',
    'fromAddress': 'From Address',
    'highProficiency': 'High Proficiency',
    'holdCount': 'Hold Count',
    'holdSeconds': 'Hold Seconds',
    'inQueueSeconds': 'In Queue Seconds',
    'isAnalytics Processed': 'Is Analytics Processed',
    'isLogged': 'Is Logged',
    'isOutbound': 'Is Outbound',
    'isRefused': 'Is Refused',
    'isShortAbandon': 'Is Short Abandon',
    'isTakeover': 'Is Takeover',
    'lastName': 'Last Name',
    'lastUpdateTime': 'Last Update Time',
    'lowProficiency': 'Low Proficiency',
    'MasterContactId': 'Master Contact ID',
    'mediaSubTypeId': 'Media SubType ID',
    'mediaSubTypeflame': 'Media SubType Flame',
    'mediaTypeId': 'Media Type ID',
    'refuseTime': 'Refuse Time',
    'mediaTypeNane': 'Media Type Name',
    'releaseSeconds': 'Release Seconds',
    'pointOfContactid': 'Point of Contact ID',
    'pointOfContactName': 'Point of Contact Name',
    'postQueueSeconds': 'Post Queue Seconds',
    'preQueueSeconds': 'Pre Queue Seconds',
    'primaryDispositionId': 'Primary Disposition ID',
    'routing Time': 'Routing Time',
    'secondaryDispositionId': 'Secondary Disposition ID',
    'serviceLevelFlag': 'Service Level Flag',
    'skillid': 'Skill ID',
    'skillName': 'Skill Name',
    'transferIndicatorId': 'Transfer Indicator ID',
    'transfer': 'Transfer',
    'IndicatorName': 'Indicator Name'
}
dashboard_df = df.rename(columns=col_mapping)
ordered_dashboard_columns = [
    "Contact Start Date", "Processed Date", "Callback Time", "Last Update Time",
    "Date AC Warehoused", "Date Contact Warehoused", "Contact ID", "Master Contact ID",
    "Campaign ID", "Campaign Name", "Agent ID", "First Name", "Last Name",
    "From Address", "Media Type ID", "Media Type Name", "Media SubType ID", "Media SubType Flame",
    "Pre Queue Seconds", "In Queue Seconds", "Post Queue Seconds", "Abandon Seconds",
    "ACW Seconds", "Agent Seconds", "Conference Seconds", "Hold Count", "Hold Seconds",
    "Release Seconds", "Refuse Time", "Routing Time", "End Reason", "Disposition Notes",
    "Primary Disposition ID", "Secondary Disposition ID", "Service Level Flag",
    "Abandoned", "Is Analytics Processed", "Analytics", "Is Logged", "Is Outbound",
    "Is Refused", "Is Short Abandon", "Is Takeover", "High Proficiency", "Low Proficiency",
    "Point of Contact ID", "Point of Contact Name", "Skill ID", "Skill Name",
    "Transfer Indicator ID", "Transfer", "Indicator Name"
]
available_columns = [col for col in ordered_dashboard_columns if col in dashboard_df.columns]
dashboard_df = dashboard_df[available_columns]

# --- Sidebar Filters (Multiple Options) ---
st.sidebar.header("Filter Options")
filtered_df = dashboard_df.copy()

# 1. Filter by Contact Start Date Range
if "Contact Start Date" in filtered_df.columns:
    filtered_df["Contact Start Date"] = pd.to_datetime(filtered_df["Contact Start Date"], errors='coerce')
    min_date = filtered_df["Contact Start Date"].min()
    max_date = filtered_df["Contact Start Date"].max()
    date_range = st.sidebar.date_input("Select Contact Start Date Range", [min_date.date(), max_date.date()])
    if len(date_range) == 2:
        start_date_filter, end_date_filter = date_range
        filtered_df = filtered_df[
            (filtered_df["Contact Start Date"].dt.date >= start_date_filter) & 
            (filtered_df["Contact Start Date"].dt.date <= end_date_filter)
        ]

# 2. Filter by Campaign Name
if "Campaign Name" in filtered_df.columns:
    campaigns = filtered_df["Campaign Name"].dropna().unique().tolist()
    selected_campaigns = st.sidebar.multiselect("Select Campaign(s)", options=campaigns, default=campaigns)
    filtered_df = filtered_df[filtered_df["Campaign Name"].isin(selected_campaigns)]

# 3. Filter by Agent ID
if "Agent ID" in filtered_df.columns:
    agents = filtered_df["Agent ID"].dropna().unique().tolist()
    selected_agents = st.sidebar.multiselect("Select Agent(s)", options=agents, default=agents)
    filtered_df = filtered_df[filtered_df["Agent ID"].isin(selected_agents)]

# 4. Filter by End Reason
if "End Reason" in filtered_df.columns:
    reasons = filtered_df["End Reason"].dropna().unique().tolist()
    selected_reasons = st.sidebar.multiselect("Select End Reason(s)", options=reasons, default=reasons)
    filtered_df = filtered_df[filtered_df["End Reason"].isin(selected_reasons)]

# 5. Filter by Call Type (Inbound/Outbound)
if "Is Outbound" in filtered_df.columns:
    call_type_mapping = {False: "Inbound", True: "Outbound", 0: "Inbound", 1: "Outbound"}
    filtered_df["Call Type"] = filtered_df["Is Outbound"].map(call_type_mapping)
    selected_call_types = st.sidebar.multiselect("Select Call Type", options=filtered_df["Call Type"].unique().tolist(), 
                                                   default=filtered_df["Call Type"].unique().tolist())
    filtered_df = filtered_df[filtered_df["Call Type"].isin(selected_call_types)]
    
# 6. Filter by Skill Name
if "Skill Name" in filtered_df.columns:
    skills = filtered_df["Skill Name"].dropna().unique().tolist()
    selected_skills = st.sidebar.multiselect("Select Skill(s)", options=skills, default=skills)
    filtered_df = filtered_df[filtered_df["Skill Name"].isin(selected_skills)]
    
# 7. Filter by Call Duration (Agent Seconds)
if "Agent Seconds" in filtered_df.columns:
    filtered_df["Agent Seconds"] = pd.to_numeric(filtered_df["Agent Seconds"], errors='coerce')
    min_duration = int(filtered_df["Agent Seconds"].min())
    max_duration = int(filtered_df["Agent Seconds"].max())
    duration_range = st.sidebar.slider("Select Agent Seconds Range", min_value=min_duration, max_value=max_duration, 
                                       value=(min_duration, max_duration))
    filtered_df = filtered_df[(filtered_df["Agent Seconds"] >= duration_range[0]) & 
                              (filtered_df["Agent Seconds"] <= duration_range[1])]

# 8. Filter by Pre Queue Seconds
if "Pre Queue Seconds" in filtered_df.columns:
    filtered_df["Pre Queue Seconds"] = pd.to_numeric(filtered_df["Pre Queue Seconds"], errors='coerce')
    min_pre = int(filtered_df["Pre Queue Seconds"].min())
    max_pre = int(filtered_df["Pre Queue Seconds"].max())
    pre_queue_range = st.sidebar.slider("Select Pre Queue Seconds Range", min_value=min_pre, max_value=max_pre, 
                                        value=(min_pre, max_pre))
    filtered_df = filtered_df[(filtered_df["Pre Queue Seconds"] >= pre_queue_range[0]) & 
                              (filtered_df["Pre Queue Seconds"] <= pre_queue_range[1])]

# 9. Filter by In Queue Seconds
if "In Queue Seconds" in filtered_df.columns:
    filtered_df["In Queue Seconds"] = pd.to_numeric(filtered_df["In Queue Seconds"], errors='coerce')
    min_in = int(filtered_df["In Queue Seconds"].min())
    max_in = int(filtered_df["In Queue Seconds"].max())
    in_queue_range = st.sidebar.slider("Select In Queue Seconds Range", min_value=min_in, max_value=max_in, 
                                       value=(min_in, max_in))
    filtered_df = filtered_df[(filtered_df["In Queue Seconds"] >= in_queue_range[0]) & 
                              (filtered_df["In Queue Seconds"] <= in_queue_range[1])]

# 10. Filter by Post Queue Seconds
if "Post Queue Seconds" in filtered_df.columns:
    filtered_df["Post Queue Seconds"] = pd.to_numeric(filtered_df["Post Queue Seconds"], errors='coerce')
    min_post = int(filtered_df["Post Queue Seconds"].min())
    max_post = int(filtered_df["Post Queue Seconds"].max())
    post_queue_range = st.sidebar.slider("Select Post Queue Seconds Range", min_value=min_post, max_value=max_post, 
                                         value=(min_post, max_post))
    filtered_df = filtered_df[(filtered_df["Post Queue Seconds"] >= post_queue_range[0]) & 
                              (filtered_df["Post Queue Seconds"] <= post_queue_range[1])]

# Reformat "Contact Start Date" for display (remove T and Z)
if "Contact Start Date" in filtered_df.columns:
    filtered_df["Contact Start Date"] = filtered_df["Contact Start Date"].dt.strftime("%Y-%m-%d %H:%M:%S")

st.write("### Filtered Detailed Contact Center Data")
st.dataframe(filtered_df)

# Extra: Download filtered data as CSV
csv = filtered_df.to_csv(index=False).encode('utf-8')
st.download_button(
    label="Download Filtered Data as CSV",
    data=csv,
    file_name='filtered_contact_center_data.csv',
    mime='text/csv'
)

# --- Additional Computations for Extra Charts ---
# Create additional fields for heatmap (day of week & hour)
filtered_df["Contact Start Date"] = pd.to_datetime(filtered_df["Contact Start Date"], errors='coerce')
filtered_df["Hour"] = filtered_df["Contact Start Date"].dt.hour
filtered_df["Weekday"] = filtered_df["Contact Start Date"].dt.day_name()

# ----------------- Multi-Tab Dashboard -----------------
tabs = st.tabs(["Overview", "Agent Analysis", "Campaign Analysis", "Queue Analysis", "Call Disposition", "Heatmap"])

# Tab 1: Overview
with tabs[0]:
    st.header("Overview")
    kpi_metrics = generate_kpi_metrics(df)
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Total Calls", kpi_metrics["Total Calls"])
    col2.metric("Answered Calls", kpi_metrics["Answered Calls"])
    col3.metric("Abandoned Calls", kpi_metrics["Abandoned Calls"])
    col4.metric("Abandon Rate (%)", kpi_metrics["Abandon Rate (%)"])
    col5, col6, col7, col8 = st.columns(4)
    col5.metric("Avg Agent Time (sec)", kpi_metrics["Avg Agent Time (sec)"])
    col6.metric("Avg Hold Time (sec)", kpi_metrics["Avg Hold Time (sec)"])
    col7.metric("Avg Wait Time (sec)", kpi_metrics["Avg Wait Time (sec)"])
    col8.metric("Avg Call Duration (sec)", kpi_metrics["Avg Call Duration (sec)"])
    
    ts_df = generate_time_series(df)
    if not ts_df.empty:
        fig_ts = px.line(ts_df, x='minute_interval', y='Calls', title="Calls Over Time")
        st.plotly_chart(fig_ts, use_container_width=True)
    summary_df = generate_summary(df)
    st.dataframe(summary_df.style.background_gradient(cmap='viridis'))

# Tab 2: Agent Analysis
with tabs[1]:
    st.header("Agent Analysis")
    top_agents_df = generate_top_agents(dashboard_df)
    if not top_agents_df.empty:
        fig_agents = px.bar(top_agents_df, x='Agent ID', y='Calls', title="Top 5 Agents by Call Count", text='Calls')
        # Capture click events for drill-down
        selected_agent = plotly_events(fig_agents, click_event=True)
        st.plotly_chart(fig_agents, use_container_width=True)
        if selected_agent:
            agent_id = selected_agent[0]['x']
            st.write(f"#### Details for Agent {agent_id}")
            agent_calls_df = dashboard_df[dashboard_df["Agent ID"] == agent_id]
            st.dataframe(agent_calls_df)
            ts_agent = generate_time_series(agent_calls_df)
            if not ts_agent.empty:
                fig_agent_ts = px.line(ts_agent, x='minute_interval', y='Calls',
                                       title=f"Calls Over Time for Agent {agent_id}")
                st.plotly_chart(fig_agent_ts, use_container_width=True)
    else:
        st.write("No agent data available.")

# Tab 3: Campaign Analysis
with tabs[2]:
    st.header("Campaign Analysis")
    camp_df = generate_campaign_distribution(df)
    if not camp_df.empty:
        fig_camp = px.bar(camp_df, x='campaignName', y='Calls', title="Calls by Campaign", text='Calls')
        st.plotly_chart(fig_camp, use_container_width=True)
        st.dataframe(camp_df)
    else:
        st.write("No campaign data available.")

# Tab 4: Queue Analysis
with tabs[3]:
    st.header("Queue Analysis")
    if all(col in filtered_df.columns for col in ["Pre Queue Seconds", "In Queue Seconds", "Post Queue Seconds"]):
        avg_pre = filtered_df["Pre Queue Seconds"].astype(float).mean()
        avg_in = filtered_df["In Queue Seconds"].astype(float).mean()
        avg_post = filtered_df["Post Queue Seconds"].astype(float).mean()
        queue_metrics_df = pd.DataFrame({
            "Metric": ["Pre Queue Seconds", "In Queue Seconds", "Post Queue Seconds"],
            "Average Seconds": [avg_pre, avg_in, avg_post]
        })
        fig_queue = px.bar(queue_metrics_df, x="Metric", y="Average Seconds", title="Average Queue Times")
        st.plotly_chart(fig_queue, use_container_width=True)
        st.dataframe(queue_metrics_df)
    else:
        st.write("Queue metrics data not available.")

# Tab 5: Call Disposition
with tabs[4]:
    st.header("Call Disposition")
    er_df = generate_end_reason_distribution(df)
    if not er_df.empty:
        fig_er = px.pie(er_df, names='endReason', values='Calls', title="Call Disposition Distribution")
        st.plotly_chart(fig_er, use_container_width=True)
        st.dataframe(er_df)
    else:
        st.write("No call disposition data available.")

# Tab 6: Heatmap
with tabs[5]:
    st.header("Call Volume Heatmap")
    if "Weekday" in filtered_df.columns and "Hour" in filtered_df.columns:
        heatmap_data = filtered_df.pivot_table(index="Weekday", columns="Hour", values="Contact ID", aggfunc="count", fill_value=0)
        # Reorder weekdays
        weekdays_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        heatmap_data = heatmap_data.reindex(weekdays_order)
        fig_heatmap = px.imshow(heatmap_data,
                                labels=dict(x="Hour of Day", y="Day of Week", color="Call Count"),
                                title="Calls by Day and Hour")
        st.plotly_chart(fig_heatmap, use_container_width=True)
    else:
        st.write("Insufficient data for heatmap.")

# Optionally, you could add more tabs or extra drill-downs as needed.
