# Function to extract the current day count from the latest consolidated file
def extract_day_count(filename):
    try:
        # Example filename: '47days_consolidated_linux_2024-11-18.csv'
        return int(filename.split('days')[0])
    except ValueError as e:
        print(f"Error extracting day count from filename {filename}: {e}")
        return 0  # Default to 0 if extraction fails

# Connect to the SFTP server
try:
    transport = paramiko.Transport((sftp_host, sftp_port))
    transport.connect(username=sftp_username, password=sftp_password)
    sftp = paramiko.SFTPClient.from_transport(transport)

    # Create local temp directory if it doesn't exist
    if not os.path.exists(local_temp_dir):
        os.makedirs(local_temp_dir)

    # Step 1: Locate the latest consolidated file
    consolidated_files = [f for f in list_sftp_files(sftp, f"{sftp_base_dir}/Linux") if 'consolidated' in f]
    sorted_consolidated_files = sort_consolidated_files(consolidated_files)  # Sort files correctly
    if not sorted_consolidated_files:
        print("No consolidated files found.")
        exit(1)

    latest_consolidated = sorted_consolidated_files[-1]  # Get the latest file
    latest_consolidated_path = f"{sftp_base_dir}/Linux/{latest_consolidated}"
    local_consolidated_path = os.path.join(local_temp_dir, latest_consolidated)

    # Download the latest consolidated file
    download_file(sftp, latest_consolidated_path, local_consolidated_path)

    # Load the latest consolidated data
    consolidated_df = pd.read_csv(local_consolidated_path)

    # Extract the current day count from the latest consolidated file
    current_day_count = extract_day_count(latest_consolidated)

    # Step 2: Find new daily files
    daily_files = sorted([f for f in list_sftp_files(sftp, sftp_base_dir) if 'Last_24_hr_Linux_Performance_metrics' in f])

    # Only consider daily files after the last consolidated file's date
    last_consolidated_date_str = latest_consolidated.split('_')[-1].replace('.csv', '')  # Get the date part from the latest file
    last_consolidated_date = datetime.strptime(last_consolidated_date_str, '%Y-%m-%d')

    # Filter daily files based on the last consolidated date
    daily_files_to_process = [f for f in daily_files if datetime.strptime(f.split('_')[-2], '%Y-%m-%d') > last_consolidated_date]

    # Step 3: Process each daily file and merge with the consolidated file
    for daily_file in daily_files_to_process:
        daily_file_path = f"{sftp_base_dir}/Linux/{daily_file}"
        local_daily_path = os.path.join(local_temp_dir, daily_file)

        # Download the daily file
        download_file(sftp, daily_file_path, local_daily_path)

        # Load the daily data
        daily_df = pd.read_csv(local_daily_path)

        # Merge with the existing consolidated data
        consolidated_df = merge_max_values(consolidated_df, daily_df)

        # Increment the day count for the new consolidated file
        current_day_count += 1

        # Determine the new consolidated file name with the correct day count
        new_consolidated_name = f"{current_day_count}days_consolidated_linux_{daily_file.split('_')[-2]}.csv"
        new_consolidated_path = os.path.join(local_temp_dir, new_consolidated_name)

        # Save the new consolidated data locally
        consolidated_df.to_csv(new_consolidated_path, index=False)

        # Upload the new consolidated file to SFTP
        upload_file(sftp, new_consolidated_path, f"{sftp_base_dir}/Linux/{new_consolidated_name}")

    # Cleanup: Delete local temp directory
    for file in os.listdir(local_temp_dir):
        os.remove(os.path.join(local_temp_dir, file))
    os.rmdir(local_temp_dir)
    print("Local temp directory cleaned up.")

except Exception as e:
    print(f"An error occurred: {e}")
finally:
    # Close the SFTP connection
    if sftp:
        sftp.close()
    if transport:
        transport.close()
