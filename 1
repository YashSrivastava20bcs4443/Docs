import requests
import pandas as pd
import paramiko
import os
from datetime import datetime, timedelta

# Prometheus server details
prometheus_url = "https://prometheus.abl.com/api/v1/query_range"

# Define the queries for max CPU and Memory utilization
max_queries = {
    "CPU": '100 * max(1 - rate(windows_cpu_time_total{mode="idle"}[5m])) by (instance)',
    "Memory": '100 - (avg(windows_memory_available_bytes) by (instance) * 100 / avg(windows_cs_physical_memory_bytes) by (instance))'
}

# Define the queries for average CPU and Memory utilization
avg_queries = {
    "CPU": '100 - (avg by (instance) (irate(windows_cpu_time_total{job=~"Windows_Servers", mode="idle"}[1m])) * 100)',
    "Memory": '100 - (windows_os_physical_memory_free_bytes{job=~"Windows_Servers"} / (windows_cs_physical_memory_bytes{job=~"Windows_Servers"} - 0) * 100)',
    "C_Drive": '(windows_logical_disk_size_bytes{job=~"Windows_Servers", volume="C:"} - windows_logical_disk_free_bytes{job=~"Windows_Servers", volume="C:"}) / windows_logical_disk_size_bytes{job=~"Windows_Servers", volume="C:"} * 100'
}

# Get the last 24 hours
end_time = datetime.now()
start_time = end_time - timedelta(hours=24)
start_time_unix = int(start_time.timestamp())
end_time_unix = int(end_time.timestamp())

# Function to query Prometheus
def query_prometheus(query, start_time, end_time, step):
    response = requests.get(prometheus_url, params={
        'query': query,
        'start': start_time,
        'end': end_time,
        'step': step
    })
    if response.status_code == 200:
        return response.json()['data']['result']
    else:
        raise Exception(f"Query failed with status code: {response.status_code}: {response.text}")

# Function to format data for max and avg calculations
def format_data(data, metric_type):
    formatted_data = []
    for entry in data:
        instance = entry['metric']['instance']
        values = entry['values']
        for value in values:
            timestamp, usage = value
            formatted_data.append({
                "timestamp": datetime.fromtimestamp(float(timestamp)),
                "instance": instance,
                "metric_type": metric_type,
                "value": float(usage)
            })
    return formatted_data

# Collect max CPU and Memory data
max_data = []
for metric_name, query in max_queries.items():
    try:
        result = query_prometheus(query, start_time_unix, end_time_unix, 60)
        max_data.extend(format_data(result, metric_name))
    except Exception as e:
        print(f"Failed to query max {metric_name}: {e}")

# Create DataFrame for max utilization
max_df = pd.DataFrame(max_data)

# Calculate max usage
max_usage = max_df.groupby(['instance', 'metric_type'])['value'].max().reset_index()

# Collect average CPU, Memory, and C Drive utilization
avg_data = []
for metric_name, query in avg_queries.items():
    try:
        result = query_prometheus(query, start_time_unix, end_time_unix, 60)
        avg_data.extend(format_data(result, metric_name))
    except Exception as e:
        print(f"Failed to query avg {metric_name}: {e}")

# Create DataFrame for avg utilization
avg_df = pd.DataFrame(avg_data)

# Calculate average usage for CPU, Memory, and C Drive
avg_usage = avg_df.groupby(['instance', 'metric_type'])['value'].mean().reset_index()

# Merge max and avg DataFrames
merged_df = pd.merge(
    max_usage, avg_usage, 
    on=['instance', 'metric_type'], 
    suffixes=('_max', '_avg')
)

# Save the merged data to a single CSV
filename = f"C:\\Temp\\performance_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
merged_df.to_csv(filename, index=False)

print(f"Data saved to {filename}")

# SFTP details
sftp_host = '<your_sftp_host>'
sftp_port = 22
sftp_username = '<your_username>'
sftp_password = '<your_password>'
sftp_target_directory = '/Network_devices_Backup/Performance_metrics'

# Function to upload the CSV to SFTP
def upload_to_sftp(local_file_path):
    try:
        transport = paramiko.Transport((sftp_host, sftp_port))
        transport.connect(username=sftp_username, password=sftp_password)
        sftp = paramiko.SFTPClient.from_transport(transport)

        # Navigate or create target directory
        try:
            sftp.chdir(sftp_target_directory)
        except IOError:
            sftp.mkdir(sftp_target_directory)
            sftp.chdir(sftp_target_directory)

        # Upload the file
        file_name = os.path.basename(local_file_path)
        sftp.put(local_file_path, f"{sftp_target_directory}/{file_name}")
        print(f"File {file_name} uploaded successfully.")

        # Verify upload
        if file_name in sftp.listdir(sftp_target_directory):
            print(f"File {file_name} verified on server.")
        else:
            print(f"File {file_name} not found on server.")

        # Delete local file after upload
        if os.path.exists(local_file_path):
            os.remove(local_file_path)
            print(f"Local file {local_file_path} deleted.")

        # Close connection
        sftp.close()
        transport.close()

    except Exception as e:
        print(f"SFTP upload failed: {e}")

# Upload the CSV to SFTP
upload_to_sftp(filename)
