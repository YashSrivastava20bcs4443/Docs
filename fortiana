import paramiko
from elasticsearch import Elasticsearch
import json
from datetime import datetime

# Firewall details
firewalls = [
    {
        'hostname': 'firewall1_ip',
        'username': 'username1',
        'password': 'password1',
        'log_file': '/var/log/firewall1_log.log'
    },
    {
        'hostname': 'firewall2_ip',
        'username': 'username2',
        'password': 'password2',
        'log_file': '/var/log/firewall2_log.log'
    },
    # Add more firewalls as needed
]

# Initialize Elasticsearch client with HTTP
es = Elasticsearch(
    ['http://localhost:9200'],
    http_auth=('your_elasticsearch_username', 'your_elasticsearch_password')
)

# SSH and collect logs
def collect_logs(hostname, username, password, log_file, local_file):
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(hostname, username=username, password=password)
    sftp = ssh.open_sftp()
    sftp.get(log_file, local_file)
    sftp.close()
    ssh.close()

# Store logs in Elasticsearch
def store_log_in_elasticsearch(log_data):
    res = es.index(index="fortigate-logs", doc_type="_doc", body=log_data)
    print(res['result'])

# Analyze and send logs
def analyze_and_store_logs():
    for firewall in firewalls:
        local_file = f"local_{firewall['hostname']}_log_file.txt"
        collect_logs(firewall['hostname'], firewall['username'], firewall['password'], firewall['log_file'], local_file)
        
        with open(local_file, 'r') as file:
            for line in file:
                log_data = {
                    "timestamp": datetime.now().isoformat(),
                    "log_message": line.strip(),
                    "firewall": firewall['hostname']
                }
                store_log_in_elasticsearch(log_data)

if __name__ == "__main__":
    analyze_and_store_logs()
